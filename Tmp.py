project-root/
├── env/
│   ├── environment.yml        # “human” spec for local dev
│   ├── lock/                  # generated by conda-lock
│   └── pack/                  # filled by CI: env tarballs
│
├── src/yourlib/ …             # importable project code
├── cli/                       # entry-point scripts / notebooks
├── configs/                   # YAMLs that describe pipelines
└── scripts/                   # deployment & cluster helpers

  # ❶ create a strict, reproducible lockfile
mamba create -n tools -y conda-lock
conda-lock lock -f env/environment.yml --kind explicit \
               --platform linux-64 -p env/lock/linux-64.lock  # 0

# ❷ build the exact env and run tests
mamba create -p /tmp/dask25 -y --file env/lock/linux-64.lock
pip install -e .                         # your project code as editable install
pytest

# ❸ pack it into a relocatable tarball
conda pack -p /tmp/dask25 \
           -o env/pack/dask25-$(git rev-parse --short HEAD).tar.gz  # 1

#yaml
# deploy.yml
- hosts: dask_workers
  become: yes
  vars:
    env_tarball: dask25-{{ git_sha }}.tar.gz
    env_target:  /opt/dask25_{{ git_sha }}
    current_link: /opt/dask25

  tasks:
    - name: Copy packed env
      ansible.builtin.copy              # 2
        src: env/pack/{{ env_tarball }}
        dest: /tmp/{{ env_tarball }}
        mode: '0644'

    - name: Unpack to target dir
      ansible.builtin.unarchive
        src: /tmp/{{ env_tarball }}
        dest: /opt/
        remote_src: yes
        creates: "{{ env_target }}/bin/python"

    - name: Update 'current' symlink atomically
      ansible.builtin.file
        src: "{{ env_target }}"
        dest: "{{ current_link }}"
        state: link

# bash 

ansible-playbook -i inventory deploy.yml -e "git_sha=$(git rev-parse --short HEAD)"

# launch_vluster.py
from dask.distributed import SSHCluster, Client

HOSTS = ["localhost",
         "worker1","worker1",
         "worker2","worker2",
         "worker3","worker3",
         "worker4","worker4"]

REMOTE_PY = "/opt/dask25/bin/python"   # always via the symlink!

CONNECT = dict(username="dask",
               client_keys=["~/.ssh/id_ed25519"],
               known_hosts=None)

cluster = SSHCluster(
    HOSTS,
    connect_options=CONNECT,
    remote_python=REMOTE_PY,
    worker_options=dict(nthreads=1,
                        memory_limit="16GB",
                        local_directory="/tmp/dask",
                        env={"POLARS_MAX_THREADS": "12"}),
    scheduler_options=dict(port=8786,
                           dashboard_address=":8787"),
)

client = Client(cluster)

# Tag workers by physical host (see earlier answers)
client.wait_for_workers(8)
for addr, w in client.scheduler_info()["workers"].items():
    client.run(lambda dask_worker=None, tag=w["host"]:
               dask_worker.resources.update({tag: 1}),
               workers=[addr])

print("Dashboard:", cluster.dashboard_link)

# txt
Observability hooks (optional but recommended)

Structured logs – point dask.config["distributed.admin.export-format"] at json and ship the files to Loki/Grafana.

Metrics – enable the scheduler’s Prometheus endpoint (--dashboard-address :12345) and scrape it alongside node exporters.

